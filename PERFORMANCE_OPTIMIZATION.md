# Performance Optimization Analysis

## Original Code Performance Issues

### ‚ùå Sequential Processing (SLOW)

The original code processed cards **one at a time**:

```python
# OLD CODE - SEQUENTIAL
for i, raw_card in enumerate(cards_to_process, 1):
    log.info(f"Processing card {i}/{len(cards_to_process)}: {raw_card.front}")
    enhanced_card = await process_card(agent, raw_card)  # ‚Üê Waits for each card
    enhanced_cards.append(enhanced_card)
```

**Problem**: Each card must complete before the next one starts.

**Time for 100 cards** (assuming 2 seconds per API call):
- Sequential: `100 cards √ó 2 seconds = 200 seconds (3.3 minutes)`

---

## Optimized Code Performance

### ‚úÖ Parallel Batch Processing (FAST)

The new code processes multiple cards **concurrently**:

```python
# NEW CODE - PARALLEL BATCHES
async def process_batch(agent, cards):
    tasks = [process_card(agent, card) for card in cards]
    return await asyncio.gather(*tasks)  # ‚Üê All cards in batch run simultaneously

# Process in batches of 10
for i in range(0, len(cards_to_process), batch_size):
    batch = cards_to_process[i : i + batch_size]
    batch_results = await process_batch(agent, batch)
    enhanced_cards.extend(batch_results)
```

**Time for 100 cards** (with batch size of 10):
- Parallel: `(100 cards / 10 batch_size) √ó 2 seconds = 20 seconds`

### üöÄ Speed Improvement: **10x faster!**

---

## Key Optimizations Implemented

### 1. **Concurrent Processing with `asyncio.gather()`**

```python
async def process_batch(agent, cards):
    tasks = [process_card(agent, card) for card in cards]
    return await asyncio.gather(*tasks)
```

**What it does**: Creates multiple async tasks that run simultaneously, not sequentially.

**Why it's faster**: While waiting for API response for card #1, the code is already processing cards #2-10.

### 2. **Configurable Batch Size**

```python
BATCH_SIZE = 10  # Adjust based on API rate limits
```

**Benefits**:
- Prevents overwhelming the API with too many concurrent requests
- Respects rate limits (e.g., OpenAI has requests-per-minute limits)
- Can be tuned for optimal performance

**How to adjust**:
- For OpenAI GPT-4: `BATCH_SIZE = 10-20` is usually safe
- For higher tier accounts: `BATCH_SIZE = 50-100`
- For testing: `BATCH_SIZE = 5`

### 3. **Error Handling with Retries**

```python
async def process_card(agent, raw_card, retry_count=0):
    try:
        result = await agent.run(prompt)
        return enhanced_card
    except Exception as e:
        if retry_count < MAX_RETRIES:
            await asyncio.sleep(2**retry_count)  # Exponential backoff
            return await process_card(agent, raw_card, retry_count + 1)
        else:
            raise
```

**Benefits**:
- Automatically retries failed requests (network errors, rate limits)
- Exponential backoff prevents hammering the API
- Graceful degradation: continues processing other batches if one fails

### 4. **Batch-Level Error Isolation**

```python
for batch in batches:
    try:
        batch_results = await process_batch(agent, batch)
        enhanced_cards.extend(batch_results)
    except Exception as e:
        log.error(f"Error processing batch: {e}")
        continue  # ‚Üê Skip failed batch, continue with others
```

**Benefits**:
- One failed batch doesn't stop the entire process
- Partial results are still written to CSV
- Better for large datasets

---

## Performance Comparison

### Scenario: Processing 1000 flashcards

| Metric                  | Sequential (Old)     | Parallel (New)      | Improvement      |
| ----------------------- | -------------------- | ------------------- | ---------------- |
| **Processing Time**     | ~33 minutes          | ~3.3 minutes        | **10x faster**   |
| **API Calls**           | 1000 sequential      | 100 batches of 10   | Same total calls |
| **Error Resilience**    | Fails on first error | Retries + continues | Much better      |
| **Memory Usage**        | Low                  | Slightly higher     | Negligible       |
| **Rate Limit Friendly** | Yes                  | Configurable        | Tunable          |

### With Larger Batch Size (BATCH_SIZE=20)

| Metric                | Value           |
| --------------------- | --------------- |
| **Processing Time**   | ~1.7 minutes    |
| **Speed Improvement** | **~20x faster** |

**Note**: Larger batch sizes require higher API rate limits.

---

## Configuration Guide

### For Different Use Cases

#### 1. **Testing (Small Dataset)**
```python
BATCH_SIZE = 3
MAX_RETRIES = 2
limit = 10  # Only process 10 cards
```

#### 2. **Production (Standard Account)**
```python
BATCH_SIZE = 10
MAX_RETRIES = 3
limit = None  # Process all cards
```

#### 3. **Production (High-Tier Account)**
```python
BATCH_SIZE = 50
MAX_RETRIES = 3
limit = None
```

#### 4. **Rate-Limited API**
```python
BATCH_SIZE = 5  # Smaller batches
MAX_RETRIES = 5  # More retries
# Add delay between batches if needed:
await asyncio.sleep(1)  # After each batch
```

### OpenAI Rate Limits Reference

| Plan        | Requests/Min | Tokens/Min | Recommended BATCH_SIZE |
| ----------- | ------------ | ---------- | ---------------------- |
| **Free**    | 3            | 40,000     | 3                      |
| **Tier 1**  | 500          | 200,000    | 10-20                  |
| **Tier 2**  | 5,000        | 2,000,000  | 50-100                 |
| **Tier 3+** | 10,000+      | 4,000,000+ | 100-200                |

---

## Additional Optimization Opportunities

### 1. **Prompt Caching** (Future Enhancement)

```python
# Cache common prompt templates
PROMPT_TEMPLATE = """
Create example sentences for this flashcard:
Front: {front}
Back: {back}
"""

# Reuse template instead of recreating each time
prompt = PROMPT_TEMPLATE.format(front=raw_card.front, back=raw_card.back)
```

**Benefit**: Reduces string operations and memory allocations.

### 2. **Streaming Responses** (Future Enhancement)

```python
async def process_card_streaming(agent, raw_card):
    async with agent.run_stream(prompt) as result:
        async for chunk in result.stream():
            # Process chunks as they arrive
            pass
        return result.output
```

**Benefit**: Start processing results before full response is received.

### 3. **Database Integration** (Future Enhancement)

Instead of loading all cards into memory:

```python
async def process_from_database():
    async for batch in db.get_cards_in_batches(batch_size=10):
        results = await process_batch(agent, batch)
        await db.save_enhanced_cards(results)
```

**Benefit**: Handles datasets larger than available RAM.

### 4. **Progress Bar** (UX Enhancement)

```python
from tqdm.asyncio import tqdm

async def process_csv_file(...):
    with tqdm(total=len(cards_to_process)) as pbar:
        for batch in batches:
            results = await process_batch(agent, batch)
            pbar.update(len(results))
```

**Benefit**: Better visibility into processing progress.

---

## Monitoring Performance

### Add Timing Metrics

```python
import time

async def process_csv_file(...):
    start_time = time.time()

    # ... processing code ...

    elapsed = time.time() - start_time
    cards_per_second = len(enhanced_cards) / elapsed

    log.info(f"Processed {len(enhanced_cards)} cards in {elapsed:.2f}s")
    log.info(f"Throughput: {cards_per_second:.2f} cards/second")
```

### Track API Costs

```python
# Pydantic AI provides usage information
result = await agent.run(prompt)
log.info(f"Tokens used: {result.usage()}")  # Track API usage
```

---

## Summary

### What Changed

| Aspect                | Before           | After                         |
| --------------------- | ---------------- | ----------------------------- |
| **Processing**        | Sequential       | Parallel batches              |
| **Speed (100 cards)** | ~3.3 min         | ~20 sec                       |
| **Error Handling**    | Basic            | Retries + exponential backoff |
| **Rate Limiting**     | None             | Configurable batch size       |
| **Resilience**        | Fails completely | Continues on partial failures |

### Performance Gains

- **10-20x faster** for typical workloads
- **Better error recovery** with automatic retries
- **Configurable** for different API limits
- **Production-ready** with proper error isolation

### When to Use Sequential vs Parallel

**Use Sequential (old approach)** when:
- Very strict rate limits (< 3 requests/min)
- Debugging individual cards
- API charges premium for concurrent requests

**Use Parallel (new approach)** when:
- Processing > 10 cards
- Standard or higher API tier
- Time is a constraint
- **Most production use cases** ‚Üê Recommended

---

## Running the Optimized Code

```bash
# Default settings (BATCH_SIZE=10)
python agent.py

# For testing with small batch
# Edit agent.py: BATCH_SIZE = 3, limit = 10

# For production with high-tier account
# Edit agent.py: BATCH_SIZE = 50, limit = None
```

The optimized code is **backward compatible** - it works exactly like the old code, just **much faster**!
